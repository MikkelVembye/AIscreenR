---
title: "Using OpenAI's GPT API models for Title and Abstract Screening in Systematic Reviews"
output: 
  rmarkdown::html_vignette:
    number_sections: true
    toc: true
date: "`r Sys.Date()`"
bibliography: AIscreenR.bib
link-citations: yes
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Using OpenAI's GPT API models for Title and Abstract Screening in Systematic Reviews}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%",
  eval = FALSE
)

# Add options
options(pillar.sigfig = 4) # ensure tibble include 4 digits
options(tibble.width = 220)
options(dplyr.print_min = 10)
options(scipen = 20)
options(dplyr.summarise.inform = FALSE) # Avoids summarize info from tidyverse
```

<div class="warning" style='margin-left:2em; margin-right:2em; margin-bottom:2em; margin-top:2em; padding:0.1em; background-color: #d7dbdd; border: solid #bdc3c7 3px'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>Important note</b></p>
<p style='margin:1em'>
*This is work-in-progress only. For now, see [Vembye, Christensen, Mølgaard, & Schytt](https://osf.io/preprints/osf/yrhzm) [-@Vembye2024_gpt] for an overview of how and when GPT API models can be used for title and abstract (TAB) screening. Our most recent results suggest that the `gpt-4o-mini` is a good model for screening titles and abstracts with performances in many cases on par with `gpt-4`. This is a very cheap model (200 times cheaper than `gpt-4`). Therefore to lower the cost, our recommendation is always to test the performance of `gpt-4o-mini` before testing other models. For an overview of other research regarding the use of GPT API models for title and abstract screening, see Syriani et al. [-@Syriani2023; -@Syriani2024], Guo et al., [-@Guo2024], and Gargari et al. [-@Gargari2024]. On a related line of research, Alshami et al. [-@Alshami2023], Khraisha et al. [-@Khraisha2024], and Issaiy et al. [@Issaiy2024] explored using the ChatGPT web browser interface for TAB screening. Based on our experience, we think that these two lines of research should not be mixed up since the draw on different GPT models and setups.*
</p></span>
</div>

Conducting a systematic review is most often resource-demanding. A critical first step to ensure the quality of systematic reviews and meta-analyses herein involves detecting all eligible references related to the literature under review [@Polanin2019]. This entails searching all pertinent literature databases relevant to the given review, most often resulting in thousands of titles and abstracts to be screened for relevance. Manual screening hereof can be a time-consuming and tedious task. However, overlooking relevant studies at this stage can be consequential, leading to substantially biased results, if the missed studies are systematically different from the detected ones. To mitigate this resource issue, we show in this vignette how to use OpenAI's GPT (Generative Pre-trained Transformer) API (Application Programming Interface) models for title and abstract screening in R. Specifically, we demonstrate how to create a test dataset and assess whether GPT is viable as the second screener in your review [cf. @Vembye2024_gpt]. 
\ 

At this point, you might ask, why use GPT API models from R and not just do the screening with ChatGPT? The answer to this is that using R to conduct the screening with GPT API models has certain advantages: 

  1. Reviewers can easily work with a large number of references, avoiding copy-paste procedures 
  [cf. @Khraisha2024].  
  2. The total screening time can be substantially lowered relative to using the ChatGPT interface, since the screening can be done in parallel. In theory, you will be able to screen 30,000 titles and abstracts per minute.
  3. It eases model comparison.
  4. Consistency between the GPT answer for the same title and abstract can easily be tested. You can also conduct multiple screenings and make inclusion judgments based on how many times a study has been included across the given number of screenings. 
  5. When using the `gpt-4o-mini`, it is cheaper than subscribing to ChatGPT plus. 
  6. Research suggest that GPT API models and the ChatGPT yields quite different screening performances. 

That said, it is also important to stress that the GPT API models used for TAB screening should always be used carefully and be assisted by a human (human-in-the-loop). Consequently, we do not recommend to use the `AIscreenR` as a single screener, unless this an absolutely final solution. When using a GPT API model as the second screener, we also recommend that the human screening has been done before uploading RIS-file(s) to R. Thereby, it is possible to compare the screenings instantly after the GPT screening has been done. 

# Get started
For many reviewers (especially those not used to code in R), it can seem overwhelming to conduct a title and abstract screening in R. Yet to calm this feeling, it is important to stress that running your first screening only requires the following rather simple steps:

  1. Load your RIS-file data and convert it to a data frame.
  2. Handle your API key (this is only necessary the first time you screen).
  3. Make one or multiple prompts.
  4. Run the screening.

And if the human screening has been conducted (as in the example below):

  5. Analyze the screening.
  6. Resolve disagreements.
  
We will go through each of these steps one by one in the following sections. 

# Load RIS-file data and convert to data frame
At this stage, we expect that you have a pile of RIS-files, containing titles and abstracts for the references you would like to screen. You can retrieve RIS-files in several ways, either directly from the research databases, a Google Scholar search, or exported from your reference management tool, such as EndNote, Mendeley, and RefMan. Alternatively, you can export RIS-file from systematic software tool such as [EPPI-reviewer](https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=2914), [Covidence](https://www.covidence.org/), [MetaReviewer](https://www.metareviewer.org/), [`revtools`](http://www.eshackathon.org/revtools/) [@westgate2019], or whatever software you use. In the example given below, we load RIS-files extracted from the [EPPI-reviewer](https://eppi.ioe.ac.uk/cms/Default.aspx?tabid=2914). A minor advantage of extracting RIS-files from systematic software tools is that they add a unique study ID to each reference. This feature makes it easier to keep track of the screening. Yet, such IDs are automatically generated in the `tabscreen_gpt()` function if unique IDs are missing. 

## Load relevant packages
To get started, we load all relevant package that are needed to construct and run the screening.
```{r, eval = FALSE, warning=FALSE, message=FALSE}
# Loading packages 
library(AIscreenR) # Used to screen and calculate gpt vs. human performance
library(revtools)  # Used to load RIS-files
library(tibble)    # Used to work with tibbles
library(dplyr)     # Used to manipulate data
library(purrr)     # For loops 
library(future)    # Used to conduct screenings in parallel
```

## Convert RIS-files to data frames
Now, we are ready to conduct the first step of the screening, which is to load our RIS-file data into R and convert it to a data frame. This can be done by using `revtools::read_bibliography()` from `revtools`. In the below example, we use RIS-file data from a review regarding the effects of the FRIENDS preventive programme on anxiety symptoms in children and adolescents [@Filges2023; @Filges2024]. Be aware, that this data is used for illustration purposes only. Thus, no hard inferences should be drawn from this example. In the example, we assume that the humans have started screening and found a bunch of relevant and irrelevant studies from which they can create the test data. Thus, we can load the RIS-files, which we downloaded from the EPPI-reviewer, separately for ex- and included RIS-files, respectively. This enables us to track the human decision by adding a `human_code` variable, where `1` and `0` indicates whether a study should be in- og excluded. 


```{r, eval = FALSE, echo = FALSE}
# NOTE: Find the ris-files behind this vignette at https://osf.io/kfbvu/

# Loading EXCLUDED studies
ris_dat_excl <- revtools::read_bibliography("vignettes/helper-stuff/friends_excl.ris") |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |> # Using only relevant variables
  mutate(
    human_code = 0, #Tracking the human decision
    across(c(author, title, abstract), ~ na_if(., "NA"))
  )


# Loading INCLUDED studies
ris_dat_incl <- revtools::read_bibliography("vignettes/helper-stuff/friends_incl.ris") |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |>
  mutate(
    human_code = 1, #Tracking the human decision
    across(c(author, title, abstract), ~ na_if(., "NA"))
  )
```

```{r, eval = FALSE}
# Loading EXCLUDED studies
ris_dat_excl <- revtools::read_bibliography("./path/to/your/excluded_ris_file.ris") |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |> # Using only relevant variables
  mutate(
    human_code = 0, #Tracking the human decision
    across(c(author, title, abstract), ~ na_if(., "NA"))
  )


# Loading INCLUDED studies
ris_dat_incl <- revtools::read_bibliography("./path/to/your/included_ris_file.ris") |> 
  suppressWarnings() |> 
  as_tibble() |>
  select(author, eppi_id, title, abstract) |>
  mutate(
    human_code = 1, #Tracking the human decision
    across(c(author, title, abstract), ~ na_if(., "NA"))
  )
```

## Create the test dataset

We have now loaded our reference data, and can construct the test data. To speed up the test, we random sample 150 irrelevant and 50 relevant records with an title and abstract. Again, this is for illustration purposes only, since you might not necessarily have access to 50 relevant records at this stage of the screening. We recommend finding at least 10 relevant reference before constructing the test data. As further described in Vembye et al. [-@Vembye2024_gpt], we remove all records with no abstract since these can distort the accuracy of the screening. 

```{r, eval = FALSE}
set.seed(09042024)

excl_sample <- 
  ris_dat_excl |> 
  filter(!is.na(abstract)) |> 
  sample_references(150)

incl_sample <- 
  ris_dat_incl |> 
  filter(!is.na(abstract)) |> 
  sample_references(50)

test_dat <- 
  bind_rows(excl_sample, incl_sample) |> 
  mutate(
    studyid = 1:n()
  ) |> 
  relocate(studyid, .after = eppi_id)


test_dat
#>  # A tibble: 200 × 6
#>   author             eppi_id studyid title abstract human_code
#>   <chr>              <chr>     <int> <chr> <chr>         <dbl>
#> 1 Moes and Frea Dou… 9433823       1 Usin… Studies…          0
#> 2 Flemke Kimberly R  9432288       2 The … The pur…          0
#> 3 Daniels M Harry a… 9431426       3 A Me… One of …          0
#> 4 Schwartzman Mered… 9434093       4 Enha… New ana…          0
#> 5 White Stuart F an… 9434418       5 Call… OBJECTI…          0
#> 6 Chao-kai X U and … 9432240       6 A no… To deal…          0
#> 7 Todd Thomas C      9434199       7 THE … This ar…          0
#> 8 Kinoshita O and K… 9431762       8 Spec… BACKGRO…          0
#> 9 Stratton Peter an… 9434563       9 Comp… This ar…          0
#>10 Stevens Sally and… 9434158      10 Inte… The num…          0
#># ℹ 190 more rows
#># ℹ Use `print(n = ...)` to see more rows
```

# Handle your API key 
After the test data has been made, we can move on to the next step of the screening, which involves generating your API key and handle it in R.  

## Get your API key 

Before you can use the `AIscreenR` functions to screen your references, you must generate your own secret API key from OpenAI. To do so you must first ensure that you have created an account at OpenAI (*if you have not done so at this stage, you can sign up [here](https://auth0.openai.com/u/login/identifier?state=hKFo2SBqQjNHSlc1ejIyREpUb01hdDF2OHEzQy12NnJwWlFUN6Fur3VuaXZlcnNhbC1sb2dpbqN0aWTZIEJSOWJaamdKLWswNGlfWDQ2NER1OXJmVUNpVmVzVjZfo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)).* When having an account, go to [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) and press the `+ Create new secret key` button (see Figure 1 below) and give your key a name. 

\  

```{r eval=TRUE, echo=FALSE, fig.cap='*Figure 1 - Generate API key from OpenAI*'}
knitr::include_graphics("helper-stuff/API_key_pic.png")
```
\ 

When you have generated your secret API key, remember to store it safely since
you will not be able see it again. **NOTE**: *If you lose your API key, you can 
just generate a new one*. 

## Manage your API key in R

When you have retrieved your API, you could in theory add it directly to the 
`AIscreenR` functions via the `api_key` argument. Yet, this would be an improper
way to work the API key since you would easily compromise your secret key. For example, 
your API key would be disclosed when sharing your codes with others, which gives them 
access to draw on your OpenAI account. Furthermore, OpenAI will cancel the API key if they recognize 
that your API key has been compromised (e.g., if you push it to a public GitHub page). 
To overcome this issue you have several options from here. You can either work with what we call
a permanent or temporary solution. 

### Permanent solution

The easiest way to work with your API key is to permanently add it to your R environment
as an environment variable. This can be achieved with `usethis::edit_r_environ()`. In the `.Renviron`
file, write `CHATGPT_KEY=your_key` as depicted in Figure 2. After entering the API key, close and save the `.Renviron` file and restart `RStudio` (ctrl + shift + F10). From now on, the `AIscreenR` functions will use the `get_api_key()` function to retrieve your API key from your R environment automatically. By using this approach you don't have to worry more about you API key (unless you update RStudio, deliberately delete the key, or get a new computer. Then you must repeat this process).
\  

```{r eval=TRUE, echo=FALSE, fig.cap='*Figure 2 - R environment file*'}
knitr::include_graphics("helper-stuff/Renviron.png")
```
\ 

### Temporary solution

If you do not want to add you API key permanently to your R environment, you can 
use `set_api_key()`. When executing `set_api_key()`, you will see a pop-up window in
which you can enter your API key. This will add your API key as temporary environment variable. 
Consequently, when you restart RStudio, you will no longer be able to find your 
API key in your R environment. Alternatively, you can pass a decrypted key to the `set_api_key()`, like
`set_api_key(key = secret_decrypt(encrypt_key, "YOUR_SECRET_KEY_FOR_DECRYTING"))`. See the [`HTTR2`](https://httr2.r-lib.org/articles/wrapping-apis.html#basics) package for further details about this solution. 
\  

```{r eval=TRUE, echo=FALSE, fig.cap='*Figure 3 - Set API key*'}
knitr::include_graphics("helper-stuff/set_api.png")
```
\ 


# Prompting in R

The next step involves writing you make one or multiple prompts that can be used for the screening. This can be done in several ways. You can either write your prompt directly in R, as illustrated below

```{r, eval = FALSE}
prompt <- "We are screening studies for a systematic literature review. 
The topic of the systematic review is the effect of the FRIENDS preventive programme on reducing
anxiety symptoms in children and adolescents. The FRIENDS programme is a 10-session manualised 
cognitive behavioural therapy (CBT) programme which can be used as both prevention and 
treatment of child and youth anxiety.  The study should focus exclusively on this topic 
and we are exclusively searching  for studies with a treatment and a comparison group. 
For each study, I would like you to assess:  1) Is the study about the FRIENDS preventive programme?  
2) Is the study estimating an effect between a treatment and control/comparison group?"
```

Or you can just write it in Word as shown in Figures 4. 

\  

```{r eval=TRUE, echo=FALSE, fig.cap='*Figure 4 - Prompting in Word*'}
knitr::include_graphics("helper-stuff/friends_prompt.png")
```
\ 


Then you can load your prompt(s) via `readtext()` from the `readtext` package. 
```{r}
word_path <-  system.file("extdata", "word_prompt_1.docx", package = "AIscreenR")

prompt <- 
  readtext::readtext(word_path)$text |> 
      stringr::str_remove_all("\n")
```

# Run screening of titles and abstracts

Now you are actually ready to conduct the title and abstract screening via `tabscreen_gpt()`. To make the `tabscreen_gpt()` running, you need to specify the following information:

 1. The data with the title and abstract information.
 2. Your prompt(s).
 3. The variable names of variables containing the titles and abstract.
 4. The model(s) you want to use for screening. 
 
Optional

 5. The variable name of variable containing the study IDs.
 6. The number of screenings you want to run for the given model (Default is `1` but with cheap models it can be advantageous to conduct multiple screening and make inclusion judgments based on how many times a study has been included across the set of screenings).
 7. Set the request per minutes.

```{r, message=FALSE, eval=FALSE}
# Get information about whether you have access to a given model and 
# how many requests per minutes you are allow to send. 
models_rpm <- rate_limits_per_minute("gpt-4o-mini")

plan(multisession)

result_object <- 
  tabscreen_gpt(
    data = test_dat, # RIS-file data create above 
    prompt = prompt, # indicate name of the loaded prompt object
    studyid = studyid, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data
    model = "gpt-4o-mini", # Model choice
    reps = 10, # Using multiple screenings with the cheap gpt-4o-mini model
    rpm = models_rpm$requests_per_minute
  )
#> * The approximate price of the current (simple) screening will be around $0.2237.
#> Progress: ───────────────────────────────────────────────────────────────── 100%
plan(sequential)
```

We have now conducted the test screening and can get the raw results as below. 

```{r, message=FALSE, eval=FALSE}
result_object$answer_data_aggregated |> 
  select(author, human_code, final_decision_gpt, final_decision_gpt_num)
#> # A tibble: 200 × 4
#>   author                                       human_code final_decision_gpt final_decision_gpt_num
#>   <chr>                                             <dbl> <chr>                               <dbl>
#> 1 Lara Elvira and Mart<c3><ad>n-Mar<c3><ad>a …          0 Exclude                                 0
#> 2 Matsumoto Manabu                                      0 Exclude                                 0
#> 3 Jordan Ann                                            0 Exclude                                 0
#> 4 Antonova E and Hamid A and Wright B and Kum…          0 Exclude                                 0
#> 5 Iafusco Dario                                         0 Exclude                                 0
#> 6 Farrell L J and Barrett P M and Claassens S           0 Include                                 1
#> 7 Rasalingam G and Rajalingam A and Chandrada…          0 Exclude                                 0
#> 8 Chappel J N and DuPont R L                            0 Exclude                                 0
#> 9 Waldrop Deborah P                                     0 Exclude                                 0
#>10 Ioana-Eva C<c4><83>dariu and Rad Dana                 0 Exclude                                 0
#># ℹ 190 more rows
#># ℹ Use `print(n = ...)` to see more rows
```


## Screen failed requests

If might happen that you experience that some of your screenings requests failed for some transient reason, for example because the server was overloaded or something else. To recover these failed request you can use `screen_error()`, as done below. 

```{r, eval=FALSE}
result_object <- 
  result_object |> 
  screen_errors()
```

# Analyze screening
To assess whether any GPT API model is suitable as a fully second screener of the review, you can use `screen_error()`

```{r screen_stats}
screen_performance <- 
  result_object |> 
  screen_analyzer(human_decision = human_code) # state the name of the variable containing the human decision. 

screen_performance
#> # A tibble: 1 × 9
#>   promptid model        reps top_p p_agreement recall specificity incl_p criteria                                                        
#>      <int> <chr>       <int> <dbl>       <dbl>  <dbl>       <dbl>  <dbl> <chr>                                                           
#>1        1 gpt-4o-mini    10     1        0.97   0.92       0.987    0.4 Studies have been included in at least 40% of the 10 screenings.
```

## Assess results via a benchmark scheme

```{r eval=TRUE, echo=FALSE, fig.cap='*Figure 5 - Generic benchmark scheme from Vembye et al. (2024)*'}
knitr::include_graphics("helper-stuff/benchmark_scheme.png")
```

## Make judgments over multiple screenings

```{r}
incl_dist <- attr(screen_performance, "p_incl_data")
incl_dist |> select(model, recall, specificity, criteria)
#> # A tibble: 10 × 4
#>   model       recall specificity criteria                                                        
#>   <chr>        <dbl>       <dbl> <chr>                                                           
#> 1 gpt-4o-mini   0.96       0.987 Studies have been included in at least 10% of the 10 screenings.
#> 2 gpt-4o-mini   0.96       0.987 Studies have been included in at least 20% of the 10 screenings.
#> 3 gpt-4o-mini   0.94       0.987 Studies have been included in at least 30% of the 10 screenings.
#> 4 gpt-4o-mini   0.92       0.987 Studies have been included in at least 40% of the 10 screenings.
#> 5 gpt-4o-mini   0.92       0.987 Studies have been included in at least 50% of the 10 screenings.
#> 6 gpt-4o-mini   0.92       0.987 Studies have been included in at least 60% of the 10 screenings.
#> 7 gpt-4o-mini   0.92       0.987 Studies have been included in at least 70% of the 10 screenings.
#> 8 gpt-4o-mini   0.88       0.987 Studies have been included in at least 80% of the 10 screenings.
#> 9 gpt-4o-mini   0.84       0.987 Studies have been included in at least 90% of the 10 screenings.
#>10 gpt-4o-mini   0.74       0.987 Studies have been included in all of the 10 screenings.   
```


# Check and resolve disagreements 

## False excluded (by gpt)

## Getting detailed descriptions for disaggrement records

```{r}
disagree_dat <- 
  result_object$answer_data_aggregated |> 
  filter(human_code == 1, final_decision_gpt_num == 0, incl_p == 0)

plan(multisession)

result_object_detail <- 
  tabscreen_gpt(
    data = disagree_dat, # RIS-file data create above 
    prompt = prompt, # indicate name of the loaded prompt object
    studyid = studyid, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data
    model = "gpt-4o-mini", # Model choice
    rpm = models_rpm$requests_per_minute,
    decision_description = TRUE
  )
#> * The approximate price of the current (simple) screening will be around $0.391.
#> * Be aware that getting descriptive, detailed responses will substantially 
#> increase the prize of the screening relative to the noted approximate prize.
#> Progress: ───────────────────────────────────────────────────────────────── 100%

plan(sequential)
```



```{r, echo = FALSE}

# Example of abstract included by the human and excluded by gpt-4o-mini
result_object_reps10_detail$answer_data$abstract[1]
#> [1] "This study documents significant differences in alliance in a predominantly Latino sample of
#>  adolescents who either completed or dropped out of a Guided Self-Change treatment program. 
#>  Therapeutic alliance, working alliance, and patient involvement were assessed via ratings of
#>  audio-recorded segments of participants' counseling sessions. Descriptive discriminant function
#>  analysis identified working alliance goals, patient participation, and therapist warmth and 
#>  friendliness variables as significantly predictive of completion status. These results were
#>  confirmed via follow-up logistic regression analyses. The use of brief clinical tools to monitor
#>  and manage alliance among adolescents receiving treatment who are at risk for dropout is 
#>  discussed. This study documents significant differences in alliance in a predominantly Latino 
#>  sample of adolescents who either completed or dropped out of a Guided Self-Change treatment program. 
#>  Therapeutic alliance, working alliance, and patient involvement were assessed via ratings of
#>  audio-recorded segments of participants' counseling sessions. Descriptive discriminant function 
#>  analysis identified working alliance goals, patient participation, and therapist warmth and
#>  friendliness variables as significantly predictive of completion status. These results were 
#>  confirmed via follow-up logistic regression analyses. The use of brief clinical tools to monitor
#>  and manage alliance among adolescents receiving treatment who are at risk for dropout is discussed"

# Example of explanation for exclusion 
# Seems reasonable why the records was thrown out by gpt.
result_object_detail$answer_data$detailed_description[1]
#> [1] "This particular study does not meet the inclusion criteria as set out by the systematic review.
#>  While it does focus on substance abuse treatment in adolescents, it does not specifically indicate 
#>  that it assesses a family-based intervention, such as Functional Family Therapy, 
#>  Multidimensional Family Therapy, or Behavioral Family Therapy. Moreover, it does not clearly 
#>  specify that the participants are primarily in outpatient drug treatment for non-opioid drug 
#>  use and that their ages are between 11–21 years. The study is more centered around the Guided
#>  Self-Change treatment program, and its impact on factors such as therapeutic alliance,
#>  working alliance, and patient involvement. Therefore, it should be excluded from the 
#>  systematic review."
```

When re-screening disagreements, it might happen that some studies will be included, those studies could just be added to the pool of included studies. 

# Approximate price of full-scale screening

Assume now that you have decided to use a GPT model as your second screener, then you would like know how much you have to pay for the full screening. 

Approximate price of screening

```{r, eval = FALSE}
# All ris-file data
all_dat <- 
  bind_rows(ris_dat_excl, ris_dat_incl) |> 
  filter(!is.na(abstract)) |> 
  mutate(studyid = 1:n())

app_obj <- 
  approximate_price_gpt(
    data = all_dat, # Tutorial data embedded in the package
    prompt = prompt, 
    studyid = studyid, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data
    model = c("gpt-4o-mini", "gpt-4"),
    reps = c(10, 1)
  )

app_obj
#> The approximate price of the (simple) screening will be around $64.1443.

app_obj$price_dollar
#> [1] 64.1443
app_obj$price_data
#> # A tibble: 2 × 6
#>   prompt   model       iterations input_price_dollar output_price_dollar total_price_dollar
#>   <chr>    <chr>            <dbl>              <dbl>               <dbl>              <dbl>
#> 1 Prompt 1 gpt-4o-mini         10               2.99               0.111               3.11
#> 2 Prompt 1 gpt-4                1              59.9                1.11               61.0  
```

When looking at the price it is clear why you want to prioritize the use of `gpt-4o mini`. You could conduct 100 screenings with that model, and it would still be half the price of using `gpt-4`.

# References
