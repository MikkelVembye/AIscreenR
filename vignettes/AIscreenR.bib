
@article{Polanin2019,
	title = {Best practice guidelines for abstract screening large-evidence systematic reviews and meta-analyses},
	volume = {10},
	issn = {1759-2879},
	url = {https://doi.org/10.1002/jrsm.1354},
	doi = {10.1002/jrsm.1354},
	abstract = {Abstract screening is one important aspect of conducting a high-quality and comprehensive systematic review and meta-analysis. Abstract screening allows the review team to conduct the tedious but vital first step to synthesize the extant literature: winnowing down the overwhelming amalgamation of citations discovered through research databases to the citations that should be ?full-text? screened and eventually included in the review. Although it is a critical process, few guidelines have been put forth since the publications of seminal systematic review textbooks. The purpose of this paper, therefore, is to provide a practical set of best practice guidelines to help future review teams and managers. Each of the 10 proposed guidelines is explained using real-world examples or illustrations from applications. We also delineate recent experiences where a team of abstract screeners double-screened 14 923 abstracts in 89 days.},
	number = {3},
	journal = {Research Synthesis Methods},
	author = {Polanin, Joshua R and Pigott, Therese D. and Espelage, Dorothy L and Grotpeter, Jennifer K},
	month = sep,
	year = {2019},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {meta-analysis, abstract screening, best practices, information retrieval},
	pages = {330--342},
}

@article{Syriani2023,
	title = {Assessing the ability of {ChatGPT} to screen articles for systematic reviews},
	journal = {arXiv preprint arXiv:2307.06464},
	author = {Syriani, Eugene and David, Istvan and Kumar, Gauransh},
	year = {2023},
}

@article{Guo2024,
	title = {Automated paper screening for clinical reviews using large language models: {Data} analysis study},
	volume = {26},
	issn = {1438-8871},
	url = {https://www.jmir.org/2024/1/e48996},
	doi = {10.2196/48996},
	abstract = {Background: The systematic review of clinical research papers is a labor-intensive and time-consuming process that often involves the screening of thousands of titles and abstracts. The accuracy and efficiency of this process are critical for the quality of the review and subsequent health care decisions. Traditional methods rely heavily on human reviewers, often requiring a significant investment of time and resources. Objective: This study aims to assess the performance of the OpenAI generative pretrained transformer (GPT) and GPT-4 application programming interfaces (APIs) in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review data sets and comparing their performance against ground truth labeling by 2 independent human reviewers. Methods: We introduce a novel workflow using the Chat GPT and GPT-4 APIs for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the API with the screening criteria in natural language and a corpus of title and abstract data sets filtered by a minimum of 2 human reviewers. We compared the performance of our model against human-reviewed papers across 6 review papers, screening over 24,000 titles and abstracts. Results: Our results show an accuracy of 0.91, a macro F1-score of 0.60, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. The interrater variability between 2 independent human screeners was κ=0.46, and the prevalence and bias-adjusted κ between our proposed methods and the consensus-based human decisions was κ=0.96. On a randomly selected subset of papers, the GPT models demonstrated the ability to provide reasoning for their decisions and corrected their initial decisions upon being asked to explain their reasoning for incorrect classifications. Conclusions: Large language models have the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, models such as GPT-4 can enhance efficiency and lead to more accurate and reliable conclusions in medical research.},
	journal = {J Med Internet Res},
	author = {Guo, Eddie and Gupta, Mehul and Deng, Jiawen and Park, Ye-Jean and Paget, Michael and Naugler, Christopher},
	year = {2024},
	keywords = {systematic review, abstract screening, Chat GPT, classification, extract, extraction, free text, GPT, GPT-4, language model, large language models, LLM, natural language processing, NLP, nonopiod analgesia, review methodology, review methods, screening, systematic, unstructured data},
	pages = {e48996},
}

@article{Filges2023,
	title = {{PROTOCOL}: {The} {FRIENDS} preventive programme for reducing anxiety symptoms in children and adolescents: {A} systematic review},
	volume = {19},
	url = {https://doi.org/10.1002/cl2.1374},
	doi = {10.1002/cl2.1374},
	abstract = {Abstract This is the protocol for a Campbell systematic review. The objectives are as follows. The main objective of this review is to answer the following research question: What are the effects of the FRIENDS preventive programme on anxiety symptoms in children and adolescents? Further, the review will attempt to answer if the effects differ between participant age groups, participant socio-economic status, type of prevention (universal, selective or indicated), type of provider (lay or mental health provider), country of implementation (Australia or other countries) and implementation issues in relation to the booster sessions and parent sessions (implemented, partly implemented or not at all).},
	number = {4},
	journal = {Campbell Systematic Reviews},
	author = {Filges, Trine and Smedslund, Geir and Eriksen, Tine and Birkefoss, Kirsten},
	month = dec,
	year = {2023},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {e1374},
}

@article{Alshami2023,
	title = {Harnessing the power of {ChatGPT} for automating systematic review process: {Methodology}, case study, limitations, and future directions},
	volume = {11},
	issn = {2079-8954},
	doi = {10.3390/systems11070351},
	number = {7},
	journal = {Systems},
	author = {Alshami, Ahmad and Elsayed, Moustafa and Ali, Eslam and Eltoukhy, Abdelrahman E E and Zayed, Tarek},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Alshami2024},
	pages = {351},
}

@article{Khraisha2024,
	title = {Can large language models replace humans in systematic reviews? {Evaluating} {GPT}-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages},
	issn = {1759-2879},
	url = {https://doi.org/10.1002/jrsm.1715},
	doi = {10.1002/jrsm.1715},
	abstract = {Abstract Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour-intensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre-Trained Transformer (GPT)-4, the biggest LLM so far. This pre-registered study uses a ?human-out-of-the-loop? approach to evaluate GPT-4's capability in title/abstract screening, full-text review and data extraction across various literature types and languages. Although GPT-4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets ({\textasciitilde}1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced ({\textasciitilde}1:3). When screening full-text literature using highly reliable prompts, GPT-4's performance was more robust, reaching ?human-like? levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.},
	journal = {Research Synthesis Methods},
	author = {Khraisha, Qusai and Put, Sophie and Kappenberg, Johanna and Warraitch, Azza and Hadfield, Kristin},
	month = mar,
	year = {2024},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {systematic reviews, machine learning, GPT, artificial intelligence (AI), large language models (LLMs), natural language processing (NLP)},
}

@article{Gargari2024,
	title = {Enhancing title and abstract screening for systematic reviews with {GPT}-3.5 turbo},
	volume = {29},
	url = {http://ebm.bmj.com/content/29/1/69.abstract},
	doi = {10.1136/bmjebm-2023-112678},
	abstract = {After conducting a database search, the subsequent phase in the execution of systematic reviews (SRs) involves title and abstract screening.1 This stage bears significant importance and necessitates the involvement of dedicated and experienced researchers who can exhibit precision and accuracy, particularly when the search yields a substantial number of studies. Besides the qualities of experience and dedication demonstrated by the screeners, several other factors influence the quality of the screening process, such as effective team management, the adoption of a double-screening approach and, notably, the implementation of a well-structured screening design. A screening tool comprises a set of questions that must be addressed by the screeners, and these questions should adhere to the following criteria: (1) they must be objective, (2) they should be single-barrelled and (3) they should encompass questions answerable with ‘yes’, ‘no’ or ‘unsure’ responses.2 The domain of large language and transformer models has showcased a promising trajectory of advancement, consistently improving day by day. These models are trained on a vast corpora of text and possess the capability to comprehend and generate human-like text.3 A prominent example within this realm is the Generative Pre-Trained Transformer (GPT) developed by OpenAI, with the latest iteration being GPT-4 at the time of composing this discourse. GPT-4 has exhibited commendable performance across a range of human-related tasks and has surpassed its predecessor, GPT-3.5, in evaluations conducted by the company.4 This single-case study was conceived to assess the performance of GPT 3.5 in the context of title and abstract screening for SRs. To execute this task, a recently published SR titled ‘Light Therapy in Insomnia Disorder: A Systematic Review and Meta-Analysis’ was selected, and the databases were queried using the keywords stipulated in the original paper.5 Two key rationales underpinned the selection of …},
	number = {1},
	journal = {BMJ Evidence-Based Medicine},
	author = {Gargari, Omid Kohandel and Mahmoudi, Mohammad Hossein and Hajisafarali, Mahsa and Samiee, Reza},
	month = feb,
	year = {2024},
	pages = {69 LP -- 70},
}

@article{Issaiy2024,
	title = {Methodological insights into {ChatGPT}’s screening performance in systematic reviews},
	volume = {24},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-024-02203-8},
	doi = {10.1186/s12874-024-02203-8},
	abstract = {The screening process for systematic reviews and meta-analyses in medical research is a labor-intensive and time-consuming task. While machine learning and deep learning have been applied to facilitate this process, these methods often require training data and user annotation. This study aims to assess the efficacy of ChatGPT, a large language model based on the Generative Pretrained Transformers (GPT) architecture, in automating the screening process for systematic reviews in radiology without the need for training data.},
	number = {1},
	journal = {BMC Medical Research Methodology},
	author = {Issaiy, Mahbod and Ghanaati, Hossein and Kolahi, Shahriar and Shakiba, Madjid and Jalali, Amir Hossein and Zarei, Diana and Kazemian, Sina and Avanaki, Mahsa Alborzi and Firouznia, Kavous},
	year = {2024},
	pages = {78},
}

@article{westgate2019,
	title = {revtools: {An} {R} package to support article screening for evidence synthesis},
	volume = {10},
	issn = {1759-2879},
	url = {https://doi.org/10.1002/jrsm.1374},
	doi = {10.1002/jrsm.1374},
	abstract = {The field of evidence synthesis is growing rapidly, with a corresponding increase in the number of software tools and workflows to support the construction of systematic reviews, systematic maps, and meta-analyses. Despite much progress, however, a number of problems remain, including slow integration of new statistical or methodological approaches into user-friendly software, low prevalence of open-source software, and poor integration among distinct software tools. These issues hinder the utility and transparency of new methods to the research community. Here, I present revtools, an R package to support article screening during evidence synthesis projects. It provides tools for the import and deduplication of bibliographic data, screening of articles by title or abstract, and visualization of article content using topic models. The software is entirely open-source and combines command-line scripting for experienced programmers with custom-built user interfaces for casual users, with further methods to support article screening to be added over time. revtools provides free access to novel methods in an open-source environment and represents a valuable step in expanding the capacity of R to support evidence synthesis projects.},
	number = {4},
	journal = {Research Synthesis Methods},
	author = {Westgate, Martin J},
	month = dec,
	year = {2019},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {systematic review, meta-analysis, natural language processing, data visualization, topic models},
	pages = {606--614},
}

@article{Filges2015a,
	title = {Functional {Family} {Therapy} ({FFT}) for young people in treatment for non-opioid drug use: {A} systematic review},
	volume = {11},
	url = {https://doi.org/10.4073/csr.2015.14},
	doi = {10.4073/csr.2015.14},
	abstract = {This Campbell systematic review assesses the effectiveness of FFT to reduce drug abuse (cannabis, amphetamines, ecstasy, or cocaine) among young people aged 11 to 21 years. The review includes two randomised controlled trials, but summarises findings from only one study reporting on the outcome of drug use. Two studies, reported in three papers, are included. Both were conducted in the USA. Only one provides outcomes related to youth drug use. It compares the effectiveness of FFT with that of alternative treatments. The results from the one study reporting on the effect of FFT on youth drug use shows a short-term (four month) reduction in the use of cannabis, an effect that disappears in the longer term. There is a dearth of evidence on the effectiveness of FFT for the treatment of non-opioid drug use in young people. Executive Summary/Abstract BACKGROUND Youth drug use is a severe problem worldwide. Usage of cannabis, amphetamine ecstasy and cocaine, referred to here as non-opioid drugs, are strongly associated with a range of health and social problems. Functional Family Therapy (FFT) is a short-term, manual-based, behaviorally oriented family therapy program for young people with behavior problems such as drug abuse, juvenile delinquency and violence. Delivered in an outpatient setting, it aims to help young people and their families by improving family interactions and relationship functioning by addressing dysfunctional individual behavior. As with many other forms of family therapy, FFT targets young people and their families as a system. As such, it recognizes the important role of the family system in the development and treatment of young people's drug abuse problems. OBJECTIVES The main aim of this review is to evaluate the current evidence on the effects of FFT on drug abuse reduction for young people in treatment for non-opioid drug use. SEARCH METHODS A wide range of electronic bibliographic databases were searched using a relatively narrow search strategy, in July 2013. We performed extensive searches in a broad selection of government and policy databanks, grey literature databases, citations in other reviews and included primary studies, and by hand searches of relevant journals and internet searches using Google. We also corresponded with researchers in the field of FFT. No language or date restrictions were applied to the searches. SELECTION CRITERIA To be eligible for inclusion, studies must have: involved a manual-based outpatient FFT treatment for young people aged 11-21 years enrolled for non-opioid drug use; used experimental, quasi-experimental or non-randomized controlled designs; reported at least one eligible outcome variable measuring abstinence, reduction of drug use, family functioning, education or vocational involvement, retention, risk behavior or other adverse effects; not focused exclusively on treating mental disorders; and had FFT as the primary intervention. DATA COLLECTION AND ANALYSIS The literature search yielded a total of 6,719 records, which were screened for eligibility based on title and abstract. From these, 108 potentially relevant records were retrieved and screened in full text, of which 9 records were potentially relevant. Finally, two studies based on three records were included in the review. Metaanalysis was not possible because only one study provided numerical results on the effect of FFT on drug use reduction. RESULTS Two studies were included and both analyzed relative effects, comparing FFT to other interventions. Only one study provided numerical results on drug use reduction comparing FFT to two other interventions (CBT and a group intervention). The reported results indicate a positive effect favoring FFT on drug use frequency at 4-month follow up, with no statistically significant difference at 7-month follow up. AUTHORS' CONCLUSIONS There is insufficient firm evidence to allow any conclusion to be drawn on the effect of FFT for young people in treatment for non-opioid drug use. There is a need for more research, and particularly for more methodologically rigorous studies in the field of treatment for young drug users. The aim of this systematic review was to explore what is known about the effectiveness of FFT for the purpose of reducing youth drug use. The evidence found does not provide a basis for drawing conclusions about actual outcomes and impacts. Consequently, no substantive conclusion on the effectiveness can be made, neither supporting nor rejecting of the present FFT treatment approach.},
	number = {1},
	journal = {Campbell Systematic Reviews},
	author = {Filges, Trine and Andersen, Ditte and Jørgensen, Anne-Marie Klint},
	month = jan,
	year = {2015},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {1--77},
}

@article{Vembye2024_gpt,
	title = {{GPT} {API} {Models} {Can} {Function} as {Highly} {Reliable} {Second} {Screeners} of {Titles} and {Abstracts} in {Systematic} {Reviews}: {A} {Proof} of {Concept} and {Common} {Guidelines}},
	doi = {10.31219/osf.io/yrhzm},
	journal = {Open Science Framework},
	author = {Vembye, Mikkel H. and Christensen, Julian and Mølgaard, Anja B. and Schytt, Frederikk L. W.},
	year = {2024},
}

@article{Syriani2024,
	title = {Screening articles for systematic reviews with {ChatGPT}},
	volume = {80},
	issn = {2590-1184},
	url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
	doi = {10.1016/j.cola.2024.101287},
	abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82\% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.},
	journal = {Journal of Computer Languages},
	author = {Syriani, Eugene and David, Istvan and Kumar, Gauransh},
	year = {2024},
	keywords = {GPT, Empirical research, Generative AI, Large language model, Literature review, Mapping study, Screening},
	pages = {101287},
}

@article{Filges2024,
	title = {The {FRIENDS} preventive programme for reducing anxiety symptoms in children and adolescents: {A} systematic review and meta-analysis},
	volume = {20},
	url = {https://doi.org/10.1002/cl2.1443},
	doi = {10.1002/cl2.1443},
	abstract = {Abstract Background Anxiety and stress responses are often considered normative experiences, and children and adolescents may benefit from anxiety prevention programmes. One such programme is FRIENDS which is based on a firm theoretical model which addresses cognitive, physiological and behavioural processes. FRIENDS is manualised and can, thus, easily be integrated into school curriculums. Objectives What are the effects of the FRIENDS preventive programme on anxiety symptoms in children and adolescents? Do the effects differ between participant age groups, participant socio-economic status, type of prevention, type of provider, country of implementation and/or implementation issues in relation to the booster sessions and parent sessions? Search Methods The database searches were carried out in September 2023, and other sources were searched in October 2023. We searched to identify both published and unpublished literature. A date restriction from 1998 and onwards was applied. Selection Criteria The intervention was three age-appropriate preventive anxiety programmes: Fun FRIENDS, FRIENDS for Life, and My FRIENDS Youth. Primary outcome was anxiety symptoms and secondary outcome was self-esteem. Studies that used a control group were eligible, whereas qualitative approaches were not. Data Collection and Analysis The number of potentially relevant studies was 2865. Forty-two studies met the inclusion criteria. Twenty-eight studies were used in the data synthesis. Four studies had a critical risk of bias. Six studies did not report data that enabled calculation of effect sizes and standard errors. Two studies had partial overlap of data to other studies used, and two were written in Persian. Meta-analyses were conducted on each outcome separately. All analyses were inverse variance weighted using random effects statistical models. Main Results Studies came from 15 different countries. Intervention start varied from 2001 to 2016. The average number of participants analysed was 240, and the average number of controls was 212. Twenty-five comparisons reported on anxiety symptoms post-intervention. The weighted average standardised mean difference (SMD) was 0.13 (95\% CI 0.04 to 0.22). There was some heterogeneity. Twelve comparisons reported on anxiety symptoms at 12 months follow-up. The weighted average SMD was 0.31 (95\% CI 0.13 to 0.49). There was a large amount of heterogeneity. Five comparisons reported on self-esteem post-intervention with a weighted average SMD of 0.20 (95\% CI ?0.20 to 0.61) and a large amount of heterogeneity. At follow-up, we found evidence that programmes implemented by mental health providers appears to perform better than programmes implemented by teachers. The evidence was inconclusive beyond 12 months follow-up. Authors' Conclusions Our results indicate that the FRIENDS intervention may reduce anxiety symptoms in children and adolescents when reported by children and adolescents themselves. The majority of trials employed a wait-list design, implying only a few studies reported on the long-term effects of the FRIENDS intervention. Our findings suggest that the FRIENDS intervention may increase the reduction in anxiety symptoms 12 months after the intervention. This emphasises the need for future research that apply designs that allows for long-term follow-up. We are uncertain about the effects on self-esteem. The overall certainty of evidence varied from low to very low. There is a need for more rigorously conducted studies.},
	number = {4},
	journal = {Campbell Systematic Reviews},
	author = {Filges, Trine and Smedslund, Geir and Eriksen, Tine and Birkefoss, Kirsten and Kildemoes, Malene Wallach},
	month = dec,
	year = {2024},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {meta-analysis, anxiety, FRIENDS},
	pages = {e1443},
}

@article{vembye_generative_2025,
	title = {Generative pretrained transformer models can function as highly reliable second screeners of titles and abstracts in systematic reviews: {A} proof of concept and common guidelines.},
	doi = {10.1037/met0000769},
	number = {Online first},
	journal = {Psychological Methods},
	author = {Vembye, Mikkel Helding and Christensen, Julian and Mølgaard, Anja Bondebjerg and Schytt, Frederikke Lykke Witthöft},
	year = {2025},
	note = {ISBN: 1939-1463
Publisher: American Psychological Association},
}
