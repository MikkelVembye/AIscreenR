% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ftscreen.r
\name{ftscreen}
\alias{ftscreen}
\title{Full-text screening with OpenAI API models}
\usage{
ftscreen(
  file_path,
  prompt = NULL,
  protocol_file_path = NULL,
  api_key = get_api_key(),
  vector_stores_name,
  model = "gpt-4o-mini",
  top_p = 1,
  temperature = 0.7,
  decision_description = FALSE,
  assistant_name = "file assistant screening",
  assistant_description =
    "An assistant to review a file and decide if it should be included or excluded in further studies based on a protocol or prompt.",
  assistant_instructions =
    "You are an assistant that helps in reviewing files to determine their relevance based on specific protocols or prompts. Determine whether the study should be included or excluded based on the protocol or prompt provided. Be explicit about whether the study should be included or excluded in further studies based on the evaluation criteria. If the study meets the protocol criteria, then it should be included in further studies.",
  messages = TRUE,
  reps = 1,
  max_tries = 16,
  time_info = TRUE,
  token_info = TRUE,
  max_seconds = NULL,
  is_transient = gpt_is_transient,
  backoff = NULL,
  after = NULL,
  rpm = 10000,
  seed_par = NULL,
  progress = TRUE,
  incl_cutoff_upper = 0.5,
  incl_cutoff_lower = 0.4,
  force = FALSE,
  sleep_time = 8,
  ...
)
}
\arguments{
\item{file_path}{A character vector of file paths or directories to be screened. Directories will be processed recursively, with files in sub-directories being combined into a single document for screening.}

\item{prompt}{A character string containing the screening prompt. Required if \code{protocol_file_path} is not provided.}

\item{protocol_file_path}{Path to a file (.txt, .md, .pdf, .docx) containing the screening protocol.}

\item{api_key}{Your OpenAI API key. Defaults to \code{get_api_key()}.}

\item{vector_stores_name}{A name for the OpenAI vector store to be created for the screening session.}

\item{model}{Character string with the name of the completion model. Can take
multiple OpenAI models. Default = \code{"gpt-4o-mini"}.
Find available models at \url{https://platform.openai.com/docs/models}.}

\item{top_p}{An alternative to sampling with temperature, called nucleus sampling,
where the model considers the results of the tokens with top_p probability mass.
So 0.1 means only the tokens comprising the top 10\% probability mass are considered.
OpenAI recommends altering this or temperature but not both. Default is 1.}

\item{temperature}{Controls randomness: lowering results in less random completions.
As the temperature approaches zero, the model will become deterministic and repetitive. Default is 0.7.}

\item{decision_description}{Logical indicating whether to include detailed descriptions
of decisions. Default is \code{FALSE}.}

\item{assistant_name, assistant_description, assistant_instructions}{Configuration for the OpenAI assistant.}

\item{messages}{Logical indicating whether to print messages embedded in the function.
Default is \code{TRUE}.}

\item{reps}{Numerical value indicating the number of times the same
question should be sent to OpenAI's API models. This can be useful to test consistency
between answers. Default is \code{1}.}

\item{max_tries, max_seconds}{Cap the maximum number of attempts with
\code{max_tries} or the total elapsed time from the first request with
\code{max_seconds}. If neither option is supplied, it will not retry.}

\item{time_info}{Logical indicating whether the run time of each
request/question should be included in the data. Default = \code{TRUE}.}

\item{token_info}{Logical indicating whether the number of prompt and completion tokens
per request should be included in the output data. Default = \code{TRUE}.}

\item{is_transient}{A predicate function that takes a single argument
(the response) and returns \code{TRUE} or \code{FALSE} specifying whether or not
the response represents a transient error.}

\item{backoff}{A function that takes a single argument (the number of failed
attempts so far) and returns the number of seconds to wait.}

\item{after}{A function that takes a single argument (the response) and
returns either a number of seconds to wait or \code{NULL}, which indicates
that the \code{backoff} strategy should be used instead.}

\item{rpm}{Numerical value indicating the number of requests per minute (rpm)
available for the specified api key.}

\item{seed_par}{Numerical value for a seed to ensure that proper,
parallel-safe random numbers are produced.}

\item{progress}{Logical indicating whether a progress bar should be shown when running
the screening in parallel. Default is \code{TRUE}.}

\item{incl_cutoff_upper}{Numerical value indicating the probability threshold
for which a study should be included. Default is 0.5.}

\item{incl_cutoff_lower}{Numerical value indicating the probability threshold
above which studies should be checked by a human. Default is 0.4.}

\item{force}{Logical argument indicating whether to force the function to use more than
10 iterations or certain models. Default is \code{FALSE}.}

\item{sleep_time}{Time in seconds to wait between checking run status. Default is 8.}

\item{...}{Further arguments to pass to the request body.}
}
\value{
An object of class \code{gpt_ftscreen} containing:
\item{answer_data}{Row per (file × prompt × repetition × model) with decisions and cached supplementary value.}
\item{answer_data_aggregated}{Aggregated inclusion probabilities (present when multiple prompts, models, or reps).}
\item{error_data}{Rows where a decision could not be derived (if any).}
\item{run_date}{Date of execution.}
\item{n_files, n_prompts, n_models, n_combinations, n_runs}{Processing counts.}
}
\description{
Performs full-text screening with OpenAI Assistants API models over local documents
(PDF, TXT, DOCX, etc.). You can supply either a single protocol file or one/many
direct prompts; the function can repeat questions (\code{reps}) to assess answer
consistency. Native (structured) function calling is used to standardize outputs.

A two-phase workflow is used:
\enumerate{
\item Supplementary detection (precomputation): For every unique input file a
short, standalone assistant run is executed that ONLY calls the
\code{supplementary_check} tool. Its single yes/no outcome is normalized
("yes"/"no"/NA) and cached.
\item Screening runs: Each (file × prompt × repetition × model) run reuses the
cached supplementary value. The main assistant is created WITHOUT the
\code{supplementary_check} tool. This prevents drift, saves tokens,
and ensures a single authoritative value per file.
}

Fallback: If the precomputation fails for a file (result = NA), the screening
runs for that file automatically include the \code{supplementary_check} tool and
allow the model to call it inline (once per run) so a value can still be
produced.
}
\note{
The \code{answer_data_aggregated} data (only present when reps > 1) contains the following variables:
\tabular{lll}{
\bold{title} \tab \code{character} \tab The filename of the screened document. \cr
\bold{model} \tab \code{character} \tab The specific model used. \cr
\bold{promptid} \tab \code{integer} \tab The prompt ID. \cr
\bold{prompt} \tab \code{character} \tab The prompt used for screening. \cr
\bold{incl_p} \tab \code{numeric} \tab The probability of inclusion across repeated responses. \cr
\bold{final_decision_gpt} \tab \code{character} \tab The final decision: 'Include', 'Exclude', or 'Check'. \cr
\bold{final_decision_gpt_num} \tab \code{integer} \tab The final numeric decision: 1 for include/check, 0 for exclude. \cr
\bold{reps} \tab \code{integer} \tab The number of repetitions for the question. \cr
\bold{n_mis_answers} \tab \code{integer} \tab The number of missing responses. \cr
\bold{supplementary} \tab \code{character} \tab Indicates if supplementary material was detected ('yes'/'no'). \cr
\bold{longest_answer} \tab \code{character} \tab The longest detailed description from responses (if \code{decision_description = TRUE}). \cr
}
\if{html}{\out{<br>}}
The \code{answer_data} data contains the following variables:
\tabular{lll}{
\bold{studyid} \tab \code{integer} \tab The study ID of the file. \cr
\bold{title} \tab \code{character} \tab The filename of the screened document. \cr
\bold{promptid} \tab \code{integer} \tab The prompt ID. \cr
\bold{prompt} \tab \code{character} \tab The prompt used for screening. \cr
\bold{model} \tab \code{character} \tab The specific model used. \cr
\bold{iterations} \tab \code{numeric} \tab The repetition number for the question. \cr
\bold{decision_gpt} \tab \code{character} \tab The raw decision from the model ('1', '0', or '1.1'). \cr
\bold{detailed_description} \tab \code{character} \tab A detailed description of the decision (if \code{decision_description = TRUE}). \cr
\bold{supplementary} \tab \code{character} \tab Indicates if supplementary material was detected ('yes'/'no'). \cr
\bold{decision_binary} \tab \code{integer} \tab The binary decision (1 for inclusion/uncertainty, 0 for exclusion). \cr
\bold{run_time} \tab \code{numeric} \tab The time taken for the request. \cr
\bold{prompt_tokens} \tab \code{integer} \tab The number of prompt tokens used. \cr
\bold{completion_tokens} \tab \code{integer} \tab The number of completion tokens used. \cr
}
}
\examples{
\dontrun{

set_api_key()

file_path <- "path/to/your/full_text_files"
protocol_file <- "path/to/your/protocol_file.txt"

# --- Run screening using the protocol file ---
result_protocol <- ftscreen(
  file_path = file_path,
  protocol_file_path = protocol_file,
  vector_stores_name = "TestFTScreenProtocol",
  model = "gpt-4o-mini",
  decision_description = TRUE,
  reps = 1,
  assistant_instructions = "
  You are a helpful agent that reviews files to determine their relevance based on specific protocols.

  CRITICAL: You must follow this EXACT two-step process:

  STEP 1: SUPPLEMENTARY CHECK ONLY
  - Use the supplementary_check function to identify if the text contains references to
    supplementary materials, appendices, or additional information.
  - This step is ONLY about identifying supplementary content - do NOT make any inclusion decisions here.

  STEP 2: PROTOCOL EVALUATION AND INCLUSION DECISION
  - Carefully review the provided protocol criteria.
  - Evaluate the study against ALL relevant criteria in the protocol.
  - Base your decision solely on whether the study meets the protocol criteria.

  IMPORTANT REMINDERS:
  - The supplementary check is separate from the inclusion decision.
  - If the study meets the protocol criteria, then it should be included in further studies.
  - Be explicit about whether the study should be included or excluded based on the protocol evaluation.
  - Provide detailed reasoning for your decision when detailed_description is enabled"
)

print(result_protocol$answer_data)


# --- Run screening using traditional prompts ---

prompts <- c("
Does this study focus on an intervention aimed at improving children's language, 
reading/literacy, or mathematical skills?
",
"Is this study written in English?",
"Does this study involve children aged 3 to 4 years old?"
)

result_prompts <- ftscreen(
  file_path = file_path,
  prompt = prompts,
  vector_stores_name = "TestFTScreenPrompts",
  model = "gpt-4o-mini",
  decision_description = TRUE,
  reps = 1
)

print(result_prompts$answer_data)
}
}
